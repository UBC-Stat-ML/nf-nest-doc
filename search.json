[
  {
    "objectID": "10_misc.html",
    "href": "10_misc.html",
    "title": "Misc",
    "section": "",
    "text": "To declare a command line argument, say n_rounds to your nextflow script, with default argument 5, use:\nparams.n_rounds = 5\nHere is an example:\n\ninclude { crossProduct; filed; deliverables } from '../cross.nf'\ninclude { instantiate; precompile; activate } from '../pkg.nf'\ninclude { combine_csvs; } from '../combine.nf'\n\ndef julia_env = file(moduleDir/'julia_env')\ndef plot_script = file(moduleDir/'plot.jl')\n\nparams.n_rounds = 5\n\ndef variables = [\n    seed: 1..10,\n    n_chains: [10, 20], \n]\n\nworkflow {\n    compiled_env = instantiate(julia_env) | precompile\n    configs = crossProduct(variables)\n    combined = run_julia(compiled_env, configs) | combine_csvs\n    plot(compiled_env, plot_script, combined)\n}\n\nprocess run_julia {\n    input:\n        path julia_env \n        val config \n    output:\n        path \"${filed(config)}\"\n    \"\"\"\n    ${activate(julia_env)}\n\n    # run your code\n    using Pigeons \n    using CSV \n    pt = pigeons(\n            target = toy_mvn_target(1000), \n            n_chains = ${config.n_chains}, \n            seed = ${config.seed},\n            n_rounds = ${params.n_rounds})\n\n    mkdir(\"${filed(config)}\")\n    CSV.write(\"${filed(config)}/summary.csv\", pt.shared.reports.summary)\n    CSV.write(\"${filed(config)}/swap_prs.csv\", pt.shared.reports.swap_prs)\n    \"\"\"\n}\n\nprocess plot {\n    input:\n        path julia_env \n        path plot_script\n        path combined_csvs_folder \n    output:\n        path '*.png'\n        path combined_csvs_folder\n    publishDir \"${deliverables(workflow, params)}\", mode: 'copy', overwrite: true\n    \"\"\"\n    ${activate(julia_env)}\n\n    include(\"$plot_script\")\n    create_plots(\"$combined_csvs_folder\")\n    \"\"\"\n}\n\nTo run it, notice that the arguments specified by params.my_arg should be specified using --my_arg value (in contrast, nextflow’s argument use a single dash, as in -profile cluster):\n\ncd experiment_repo\n./nextflow run nf-nest/examples/params.nf -profile cluster --n_rounds 6\n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/examples/params.nf` [deadly_pasteur] DSL2 - revision: 093b673fc8\n[a1/01de97] Cached process &gt; combine_workflow:instantiate_process\n[7f/c48687] Cached process &gt; instantiate_process\n[18/b3eff4] Cached process &gt; precompile\n[62/e5f07c] Cached process &gt; combine_workflow:precompile\n[02/d406f5] Submitted process &gt; run_julia (12)\n[94/86641f] Submitted process &gt; run_julia (2)\n[84/9166ff] Submitted process &gt; run_julia (10)\n[73/859689] Submitted process &gt; run_julia (5)\n[ee/dea556] Submitted process &gt; run_julia (13)\n[2a/fce377] Submitted process &gt; run_julia (8)\n[61/a98f2b] Submitted process &gt; run_julia (4)\n[ff/7b1a68] Submitted process &gt; run_julia (11)\n[5b/d6f5cb] Submitted process &gt; run_julia (1)\n[ae/00a054] Submitted process &gt; run_julia (7)\n[04/d3b4f3] Submitted process &gt; run_julia (3)\n[2d/00705f] Submitted process &gt; run_julia (9)\n[e3/de0446] Submitted process &gt; run_julia (6)\n[43/eda189] Submitted process &gt; run_julia (14)\n[1c/474835] Submitted process &gt; run_julia (15)\n[81/49b08f] Submitted process &gt; run_julia (16)\n[c2/f05ac8] Submitted process &gt; run_julia (17)\n[6c/2f23bd] Submitted process &gt; run_julia (18)\n[11/5a5ed4] Submitted process &gt; run_julia (19)\n[f7/5c018b] Submitted process &gt; run_julia (20)\n[0e/5886b4] Submitted process &gt; combine_workflow:combine_process\n[a6/000d86] Submitted process &gt; plot\n\n\nNotice that the function deliverables(workflow, params) takes it into account so that the deliverables directory is organized correctly:\n\ntree experiment_repo/deliverables\n\nexperiment_repo/deliverables\n├── scriptName=full.nf\n│   ├── output\n│   │   ├── summary.csv\n│   │   └── swap_prs.csv\n│   ├── plot.png\n│   └── runName.txt\n└── scriptName=params.nf___n_rounds=6\n    ├── output\n    │   ├── summary.csv\n    │   └── swap_prs.csv\n    ├── plot.png\n    └── runName.txt\n\n4 directories, 8 files",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "10_misc.html#process-parameters",
    "href": "10_misc.html#process-parameters",
    "title": "Misc",
    "section": "",
    "text": "To declare a command line argument, say n_rounds to your nextflow script, with default argument 5, use:\nparams.n_rounds = 5\nHere is an example:\n\ninclude { crossProduct; filed; deliverables } from '../cross.nf'\ninclude { instantiate; precompile; activate } from '../pkg.nf'\ninclude { combine_csvs; } from '../combine.nf'\n\ndef julia_env = file(moduleDir/'julia_env')\ndef plot_script = file(moduleDir/'plot.jl')\n\nparams.n_rounds = 5\n\ndef variables = [\n    seed: 1..10,\n    n_chains: [10, 20], \n]\n\nworkflow {\n    compiled_env = instantiate(julia_env) | precompile\n    configs = crossProduct(variables)\n    combined = run_julia(compiled_env, configs) | combine_csvs\n    plot(compiled_env, plot_script, combined)\n}\n\nprocess run_julia {\n    input:\n        path julia_env \n        val config \n    output:\n        path \"${filed(config)}\"\n    \"\"\"\n    ${activate(julia_env)}\n\n    # run your code\n    using Pigeons \n    using CSV \n    pt = pigeons(\n            target = toy_mvn_target(1000), \n            n_chains = ${config.n_chains}, \n            seed = ${config.seed},\n            n_rounds = ${params.n_rounds})\n\n    mkdir(\"${filed(config)}\")\n    CSV.write(\"${filed(config)}/summary.csv\", pt.shared.reports.summary)\n    CSV.write(\"${filed(config)}/swap_prs.csv\", pt.shared.reports.swap_prs)\n    \"\"\"\n}\n\nprocess plot {\n    input:\n        path julia_env \n        path plot_script\n        path combined_csvs_folder \n    output:\n        path '*.png'\n        path combined_csvs_folder\n    publishDir \"${deliverables(workflow, params)}\", mode: 'copy', overwrite: true\n    \"\"\"\n    ${activate(julia_env)}\n\n    include(\"$plot_script\")\n    create_plots(\"$combined_csvs_folder\")\n    \"\"\"\n}\n\nTo run it, notice that the arguments specified by params.my_arg should be specified using --my_arg value (in contrast, nextflow’s argument use a single dash, as in -profile cluster):\n\ncd experiment_repo\n./nextflow run nf-nest/examples/params.nf -profile cluster --n_rounds 6\n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/examples/params.nf` [deadly_pasteur] DSL2 - revision: 093b673fc8\n[a1/01de97] Cached process &gt; combine_workflow:instantiate_process\n[7f/c48687] Cached process &gt; instantiate_process\n[18/b3eff4] Cached process &gt; precompile\n[62/e5f07c] Cached process &gt; combine_workflow:precompile\n[02/d406f5] Submitted process &gt; run_julia (12)\n[94/86641f] Submitted process &gt; run_julia (2)\n[84/9166ff] Submitted process &gt; run_julia (10)\n[73/859689] Submitted process &gt; run_julia (5)\n[ee/dea556] Submitted process &gt; run_julia (13)\n[2a/fce377] Submitted process &gt; run_julia (8)\n[61/a98f2b] Submitted process &gt; run_julia (4)\n[ff/7b1a68] Submitted process &gt; run_julia (11)\n[5b/d6f5cb] Submitted process &gt; run_julia (1)\n[ae/00a054] Submitted process &gt; run_julia (7)\n[04/d3b4f3] Submitted process &gt; run_julia (3)\n[2d/00705f] Submitted process &gt; run_julia (9)\n[e3/de0446] Submitted process &gt; run_julia (6)\n[43/eda189] Submitted process &gt; run_julia (14)\n[1c/474835] Submitted process &gt; run_julia (15)\n[81/49b08f] Submitted process &gt; run_julia (16)\n[c2/f05ac8] Submitted process &gt; run_julia (17)\n[6c/2f23bd] Submitted process &gt; run_julia (18)\n[11/5a5ed4] Submitted process &gt; run_julia (19)\n[f7/5c018b] Submitted process &gt; run_julia (20)\n[0e/5886b4] Submitted process &gt; combine_workflow:combine_process\n[a6/000d86] Submitted process &gt; plot\n\n\nNotice that the function deliverables(workflow, params) takes it into account so that the deliverables directory is organized correctly:\n\ntree experiment_repo/deliverables\n\nexperiment_repo/deliverables\n├── scriptName=full.nf\n│   ├── output\n│   │   ├── summary.csv\n│   │   └── swap_prs.csv\n│   ├── plot.png\n│   └── runName.txt\n└── scriptName=params.nf___n_rounds=6\n    ├── output\n    │   ├── summary.csv\n    │   └── swap_prs.csv\n    ├── plot.png\n    └── runName.txt\n\n4 directories, 8 files",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "10_misc.html#dry-runs",
    "href": "10_misc.html#dry-runs",
    "title": "Misc",
    "section": "Dry runs",
    "text": "Dry runs\nA useful application of process parameter is a “dry run switch” for doing a quick version of the pipeline to help quickly debugging.\nHere is an example below. Notice that we pass the dry run option to crossProduct(); instead of emitting all values in the cross product, it will only emit one:\n\ninclude { crossProduct; filed; deliverables } from '../cross.nf'\ninclude { instantiate; precompile; activate } from '../pkg.nf'\n\ndef julia_env = file(moduleDir/'julia_env')\n\nparams.dryRun = false\nparams.n_rounds = params.dryRun ? 1 : 5\n\ndef variables = [\n    seed: 1..10,\n    n_chains: [10, 20], \n]\n\nworkflow {\n    compiled_env = instantiate(julia_env) | precompile\n    configs = crossProduct(variables, params.dryRun)\n    run_julia(compiled_env, configs) \n}\n\nprocess run_julia {\n    input:\n        path julia_env \n        val config \n    \"\"\"\n    ${activate(julia_env)}\n\n    # run your code\n    using Pigeons \n    using CSV \n    pt = pigeons(\n            target = toy_mvn_target(1000), \n            n_chains = ${config.n_chains}, \n            seed = ${config.seed},\n            n_rounds = ${params.n_rounds})\n    \"\"\"\n}\n\nTo run it in dry run model:\n\ncd experiment_repo\n./nextflow run nf-nest/examples/dry_run.nf -profile cluster --dryRun\n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/examples/dry_run.nf` [spontaneous_colden] DSL2 - revision: c7304bca14\n[7f/c48687] Cached process &gt; instantiate_process\n[18/b3eff4] Cached process &gt; precompile\n[a3/b887a1] Submitted process &gt; run_julia (1)",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "10_misc.html#what-to-commit",
    "href": "10_misc.html#what-to-commit",
    "title": "Misc",
    "section": "What to commit?",
    "text": "What to commit?\nStandard git guidelines suggest to never commit “derived files”. We recommend to deviate slightly from git conventions and commit a bit more than just strict minimum:\n\nManifest.toml is derived from Project.toml, but commit it, as it contains precise version information needed for reproducibility (in contrast to package developers, who would only commit Project.toml, but numerical experiments is not the same as a package!)\nOnce there are experiments you plan to include in a paper, commit the corresponding sub-folder of deliverables, including the CSV files used to produce that figure. This way the tex repo can just use the experiment repo as a submodule and the authors can compile the paper right away. Having the CSV there also mean that plot esthetics can be quickly tweaked later on (e.g., the night before a talk).",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "10_misc.html#updating-code",
    "href": "10_misc.html#updating-code",
    "title": "Misc",
    "section": "Updating code",
    "text": "Updating code\nNumerical experiment are often based on code you are developing along the way. When the code is updated, with a bit of organization, nextflow can figure out which subset of the workflow needs to be re-run. We present two models for doing this: one for lightweight code such as plotting/analysis, and one for more substantial code, e.g. a method you are developing.\n\nLightweight code\nInclude the .jl file in the nextflow repo, feed it to the node as input, and use a Julia include() on it. We have already used that pattern for the plot node in an earlier example.\nIf you have several Julia files, put them in a directory, and use the syntax:\n\ninclude { activate; } from \"../pkg.nf\"\n\ndef julia_files = file(moduleDir/\"julia/*.jl\")\n\nworkflow {\n    run_julia(julia_files)\n}\n\nprocess run_julia {\n    debug true\n    input:\n        file julia_files\n    \"\"\"\n    ${activate()}\n    include(\"a.jl\")\n    include(\"b.jl\")\n    \"\"\"\n}\n\n\ncd experiment_repo\n./nextflow run nf-nest/examples/includes.nf \n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/examples/includes.nf` [loquacious_edison] DSL2 - revision: f263a35a46\n[34/6a94ea] Submitted process &gt; run_julia\nhello\nworld\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPassing a directory as input (rather than a collection of files as done above), is not ideal in this context. This is because nextflow does not currently recurse inside the directory to compute the checksum used to determine if the cache can be used. Recall that in unix, the change date of a directory only changes when a file is deleted or added under it, and not when a file under it is edited!\n\n\n\n\nLibrary\nIf the code you include is more complex, and/or might be used outside of the context of one nextflow script, it is better to package it.\nIn Julia, creating a package is very simple and it can be published right away and for free on github: see this tutorial.\nFor example, we wrote a small Julia package, CombineCSV.jl to perform the CSV combination in this earlier section.\nTo add or update, you can use a script of that form:\nENV[\"JULIA_PKG_PRECOMPILE_AUTO\"]=0\nusing Pkg \nPkg.activate(\"julia_env\")\nPkg.add(url = \"https://github.com/UBC-Stat-ML/CombineCSVs\")\nwhere you would replace the URL by the git repo you are using. Note that add will also update to the head of the main branch.\nThis updates the “Manifest.toml” file which in turns signal our “pkg.jl” instantiate utility that it needs to be reran by nextflow:\n\nparams.nPrecompileThreads = 10\n\nparams.julia_env = 'julia_env'\njulia_env = file(params.julia_env)\njulia_env.mkdir()\n\n// Can be used as standalone, but typically used inside a user nf file\nworkflow  {\n    instantiate(julia_env) | precompile\n}\n\ndef instantiate(julia_env) { instantiate_process(julia_env, file(julia_env/\"Manifest.toml\"))}\n\nprocess instantiate_process {\n    executor 'local' // we need internet access\n    scratch false // we want changes in Manifest.toml to be saved\n    input: \n        path julia_env\n        path toml // needed for correct cache behaviour under updates\n    output:\n        path julia_env\n\n    \"\"\"\n    ${activate(julia_env)}\n\n    ENV[\"JULIA_PKG_PRECOMPILE_AUTO\"]=0\n    using Pkg\n    Pkg.instantiate()\n    \"\"\"\n}\n\nprocess precompile {\n    input:\n        path julia_env\n    output:\n        path julia_env\n    cpus params.nPrecompileThreads \n    memory 15.GB\n    \"\"\"\n    ${activate(julia_env, params.nPrecompileThreads)}\n\n    using Pkg \n    Pkg.offline(true) \n    Pkg.precompile()\n    \"\"\"\n}\n\n// Start Julia and with the provided environment and optionally, number of threads (1 by default) \n// Needs to be the very first line of the process script\ndef activate(julia_env, nThreads = 1) {\n    return \"#!/usr/bin/env julia --threads=${nThreads} --project=$julia_env\"\n}\n\ndef activate() {\n    return \"#!/usr/bin/env julia --threads=1\"\n}",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "10_misc.html#report",
    "href": "10_misc.html#report",
    "title": "Misc",
    "section": "Report",
    "text": "Report\nFollowing the nextflow documentation, we have set nextflow.config so that report.html, timeline.html and dag.html are automatically created.\nTo preview them in VS Code, add a VS Code Extension allowing html preview, for example Live Server. Then right click on the html file and select Show Preview.",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "08_containers.html",
    "href": "08_containers.html",
    "title": "Containers",
    "section": "",
    "text": "Containers such as Docker and apptainer address two issues:\n\nmaking pipeline reproducible,\nwe do not have root access in HPC.",
    "crumbs": [
      "Containers"
    ]
  },
  {
    "objectID": "08_containers.html#overview",
    "href": "08_containers.html#overview",
    "title": "Containers",
    "section": "",
    "text": "Containers such as Docker and apptainer address two issues:\n\nmaking pipeline reproducible,\nwe do not have root access in HPC.",
    "crumbs": [
      "Containers"
    ]
  },
  {
    "objectID": "08_containers.html#using-an-existing-container",
    "href": "08_containers.html#using-an-existing-container",
    "title": "Containers",
    "section": "Using an existing container",
    "text": "Using an existing container\nIn the file nextflow.config provided in nf-nest we provide an example of container configuration:\n\nhead -n 28 experiment_repo/nextflow.config\n\nprofiles {\n    standard {  \n        docker.enabled = true\n        process {\n            withLabel:containerized {\n                container = 'alexandrebouchardcote/default:0.1.6'\n            }\n        }\n    }\n    cluster {\n        apptainer.enabled = true\n        process {\n            scratch = true\n            executor = 'slurm'\n            cpus = 1\n            memory = 4.GB\n            time = '2h'\n            clusterOptions = \"--nodes=1 --account $ALLOCATION_CODE\"\n            withLabel:containerized {\n                container = 'alexandrebouchardcote/default:0.1.6'\n                module = 'apptainer'\n            }\n            withLabel:gpu {\n                clusterOptions = \"--nodes=1 --account ${ALLOCATION_CODE}-gpu --gpus 1\"\n            }\n        }\n    }\n}\n\n\nThere are two profiles, the default one using Docker, and the cluster one, using apptainer. We define a label for this container, which we call here containerized.\nTo instruct one of the process in the workflow to use that container, add the directive label 'containerized'.\nHere is an example of a process to compile and run a Stan program, where Stan is part of the above container:\n\nprocess compile_stan {\n    label 'containerized' \n\n    input:\n        path stan_file\n    output:\n        path \"${stan_file.baseName}\"\n        \n    \"\"\"\n    # name of stan file without extension\n    base_name=`basename $stan_file .stan`\n\n    # need to run Stan's make file from the CMDSTAN dir\n    CUR=`pwd`\n    cd \\$CMDSTAN\n    make \\$CUR/\\$base_name\n    \"\"\"\n}\n\nprocess run_stan {\n    label 'containerized' \n\n    input:\n        path stan_exec \n        path data\n\n    \"\"\"\n    ./$stan_exec sample \\\n        data file=$data \\\n        output file=samples.csv \n\n    # Compute ESS from Stan's stansummary utility\n    \\$CMDSTAN/bin/stansummary samples.csv --csv_filename ess.csv\n    \"\"\"\n}\n\nHere is an example workflow using these containerized processes:\n\ninclude { compile_stan; run_stan; } from \"./stan.nf\"\n\ndef stanModel = file(\"https://raw.githubusercontent.com/Julia-Tempering/Pigeons.jl/refs/heads/main/examples/stan/bernoulli.stan\")\ndef data = file(\"https://raw.githubusercontent.com/Julia-Tempering/Pigeons.jl/refs/heads/main/examples/stan/bernoulli.data.json\")\n\nworkflow {\n    compiled = compile_stan(stanModel)\n    run_stan(compiled, data)\n}\n\nTo run it:\n\ncd experiment_repo\n./nextflow run nf-nest/examples/stan_example.nf -profile cluster\n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/examples/stan_example.nf` [sleepy_kalman] DSL2 - revision: 8cef9f29d6\nWARN: Apptainer cache directory has not been defined -- Remote image will be stored in the path: /arc/burst/st-alexbou-1/abc/nf-nest-doc/experiment_repo/work/singularity -- Use the environment variable NXF_APPTAINER_CACHEDIR to specify a different location\nPulling Apptainer image docker://alexandrebouchardcote/default:0.1.6 [cache /arc/burst/st-alexbou-1/abc/nf-nest-doc/experiment_repo/work/singularity/alexandrebouchardcote-default-0.1.6.img]\n[17/1a0ced] Submitted process &gt; compile_stan\n[c3/ba0ad3] Submitted process &gt; run_stan",
    "crumbs": [
      "Containers"
    ]
  },
  {
    "objectID": "08_containers.html#creating-containers",
    "href": "08_containers.html#creating-containers",
    "title": "Containers",
    "section": "Creating containers",
    "text": "Creating containers\nThere are many reference online for creating containers in general, but much less on creating cross-platform containers working on both x86 and Apple Silicon.\nWe have created some script to help doing this, see code and instructions at this page.",
    "crumbs": [
      "Containers"
    ]
  },
  {
    "objectID": "06_cross_product.html",
    "href": "06_cross_product.html",
    "title": "Job cross products",
    "section": "",
    "text": "Suppose you are interested in running a piece of code with many different inputs, with each execution performed on a different compute node of a cluster.\nThis page shows a streamlined way to do so.",
    "crumbs": [
      "Job cross products"
    ]
  },
  {
    "objectID": "06_cross_product.html#overview",
    "href": "06_cross_product.html#overview",
    "title": "Job cross products",
    "section": "",
    "text": "Suppose you are interested in running a piece of code with many different inputs, with each execution performed on a different compute node of a cluster.\nThis page shows a streamlined way to do so.",
    "crumbs": [
      "Job cross products"
    ]
  },
  {
    "objectID": "06_cross_product.html#example",
    "href": "06_cross_product.html#example",
    "title": "Job cross products",
    "section": "Example",
    "text": "Example\nAs a toy example, suppose we want to compute all additions of the form a + b where a and b are integers from 1 to 3. In addition, we also want a * b. This means we will need \\(3 \\times 3 \\times 2\\) calculations.\nWe can characterize the inputs a the cross-product denoted \\(\\{1, 2, 3\\} \\times \\{1, 2, 3\\} \\times \\{+, *\\}\\).",
    "crumbs": [
      "Job cross products"
    ]
  },
  {
    "objectID": "06_cross_product.html#nextflow-script",
    "href": "06_cross_product.html#nextflow-script",
    "title": "Job cross products",
    "section": "Nextflow script",
    "text": "Nextflow script\nThe script below will perform the following operations.\n\n// we use utilities in the nf-nest submodule\n// in user scripts, path would be './nf-nest/cross.nf' \ninclude { crossProduct; filed; deliverables } from '../cross.nf'\ninclude { instantiate; precompile; activate } from '../pkg.nf'\n\ndef variables = [\n    first: 1..3,\n    second: 1..3,\n    operation: [\"+\", \"*\"]\n]\n\n// specifies the order of operations\nworkflow {\n    // look at all combinations of variables\n    configs = crossProduct(variables)\n    // run Julia on 18 nodes!\n    run_julia(configs)\n\n    // equivalent syntax:\n    // crossProduct(variables) | run_julia\n}\n\nprocess run_julia {\n    debug true // by default, standard out is not shown, use this to show it\n    \n    // information used when submitting job to queue\n    time 2.min\n    cpus 1 \n    memory 5.GB\n\n    input:\n        val config \n    \"\"\"\n    ${activate()}\n    # ^ this is just a shortcut for:\n    #!/usr/bin/env julia --threads=1\n\n    @show ${config.first} ${config.operation} ${config.second}\n    \"\"\"\n}\n\nFor more information:\n\nRead on the nextflow scripting language\nRead on nextflow’s process block\nconfigs in the above is an example of a nextflow Channel\nMore on nextflow’s workflow block",
    "crumbs": [
      "Job cross products"
    ]
  },
  {
    "objectID": "06_cross_product.html#running-the-script",
    "href": "06_cross_product.html#running-the-script",
    "title": "Job cross products",
    "section": "Running the script",
    "text": "Running the script\nRunning it with the -profile cluster option will:\n\nbuild a cross-product from variables\nfor each one, automatically create submission scripts\nrun these Julia processes and show the standard out.\n\nFrom the command line, running the script is done as follows:\n\ncd experiment_repo\n./nextflow run nf-nest/examples/many_jobs.nf -profile cluster\n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/examples/many_jobs.nf` [tiny_lagrange] DSL2 - revision: aa082b1978\n[77/ad410f] Submitted process &gt; run_julia (9)\n[0e/773f47] Submitted process &gt; run_julia (13)\n[a7/945d98] Submitted process &gt; run_julia (3)\n[ef/ba4162] Submitted process &gt; run_julia (6)\n[25/747142] Submitted process &gt; run_julia (10)\n[cc/2f94b3] Submitted process &gt; run_julia (7)\n[12/c46b9a] Submitted process &gt; run_julia (4)\n[46/348baf] Submitted process &gt; run_julia (8)\n[60/df2e03] Submitted process &gt; run_julia (12)\n[8b/5792ce] Submitted process &gt; run_julia (11)\n[5e/f5689d] Submitted process &gt; run_julia (2)\n[13/625db4] Submitted process &gt; run_julia (5)\n[06/cdbf65] Submitted process &gt; run_julia (1)\n[9e/31e73e] Submitted process &gt; run_julia (14)\n[ad/25ff98] Submitted process &gt; run_julia (15)\n[36/382458] Submitted process &gt; run_julia (16)\n[f0/634114] Submitted process &gt; run_julia (17)\n[db/921b35] Submitted process &gt; run_julia (18)\n2 + 2 = 4\n1 + 2 = 3\n3 + 1 = 4\n1 * 3 = 3\n2 * 2 = 4\n2 + 1 = 3\n1 * 2 = 2\n2 * 1 = 2\n2 * 3 = 6\n2 + 3 = 5\n1 * 1 = 1\n1 + 3 = 4\n1 + 1 = 2\n3 * 1 = 3\n3 + 2 = 5\n3 * 2 = 6\n3 + 3 = 6\n3 * 3 = 9",
    "crumbs": [
      "Job cross products"
    ]
  },
  {
    "objectID": "06_cross_product.html#filtering",
    "href": "06_cross_product.html#filtering",
    "title": "Job cross products",
    "section": "Filtering",
    "text": "Filtering\nIn some case we want to run only a subset of the cross product. For example, suppose we want only the runs of the form a * a and a + a. This can be done using the filter() function in nextflow:\n\n// we use utilities in the nf-nest submodule\n// in user scripts, path would be './nf-nest/cross.nf' \ninclude { crossProduct; filed; deliverables } from '../cross.nf'\ninclude { instantiate; precompile; activate } from '../pkg.nf'\n\ndef variables = [\n    first: 1..3,\n    second: 1..3,\n    operation: [\"+\", \"*\"]\n]\n\n// specifies the order of operations\nworkflow {\n    configs = crossProduct(variables).filter{ config -&gt; config.first == config.second }\n    run_julia(configs)\n\n    // equivalent pipe syntax:\n    // crossProduct(variables) | filter{ config -&gt; config.first == config.second } | run_julia\n}\n\nprocess run_julia {\n    debug true // by default, standard out is not shown, use this to show it\n    input:\n        val config \n    \"\"\"\n    ${activate()}\n\n    @show ${config.first} ${config.operation} ${config.second}\n    \"\"\"\n}\n\nRunning this\n\ncd experiment_repo\n./nextflow run nf-nest/examples/filter.nf  \n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/examples/filter.nf` [gigantic_heisenberg] DSL2 - revision: d9de661ecc\n[9c/40cdd5] Submitted process &gt; run_julia (3)\n[4c/c74cef] Submitted process &gt; run_julia (2)\n[14/ba5583] Submitted process &gt; run_julia (4)\n[9e/741be8] Submitted process &gt; run_julia (6)\n[c8/236bfe] Submitted process &gt; run_julia (1)\n[84/fa064c] Submitted process &gt; run_julia (5)\n2 + 2 = 4\n1 * 1 = 1\n3 * 3 = 9\n2 * 2 = 4\n1 + 1 = 2\n3 + 3 = 6",
    "crumbs": [
      "Job cross products"
    ]
  },
  {
    "objectID": "06_cross_product.html#log-scales",
    "href": "06_cross_product.html#log-scales",
    "title": "Job cross products",
    "section": "Log scales",
    "text": "Log scales\nTo create a list of parameters in log scale, use:\n(0..2).collect{ i -&gt; Math.pow(2.0, i)}\nThis will return 2.0^0, 2.0^1, 2.0^2.",
    "crumbs": [
      "Job cross products"
    ]
  },
  {
    "objectID": "04_launch.html",
    "href": "04_launch.html",
    "title": "Launching an experiment",
    "section": "",
    "text": "We show how to launch an “experiment”, i.e., a nextflow script.\nWe cover two ways to launch an experiment:\n\nLocal: where all processes (nodes in the workflow graph) run in the same machine.\nCluster where each node in the graph can run in different machines in a cluster.\n\nMethod 1 is useful to run experiments on a laptop and for prototyping.\nSurprisingly, thanks to nextflow, method 2 only involves adding the command line option -profile cluster. This is because nextflow takes care of generating submission scripts, transferring files and orchestring everything.",
    "crumbs": [
      "Launch an experiment"
    ]
  },
  {
    "objectID": "04_launch.html#overview",
    "href": "04_launch.html#overview",
    "title": "Launching an experiment",
    "section": "",
    "text": "We show how to launch an “experiment”, i.e., a nextflow script.\nWe cover two ways to launch an experiment:\n\nLocal: where all processes (nodes in the workflow graph) run in the same machine.\nCluster where each node in the graph can run in different machines in a cluster.\n\nMethod 1 is useful to run experiments on a laptop and for prototyping.\nSurprisingly, thanks to nextflow, method 2 only involves adding the command line option -profile cluster. This is because nextflow takes care of generating submission scripts, transferring files and orchestring everything.",
    "crumbs": [
      "Launch an experiment"
    ]
  },
  {
    "objectID": "04_launch.html#example-nextflow-script",
    "href": "04_launch.html#example-nextflow-script",
    "title": "Launching an experiment",
    "section": "Example nextflow script",
    "text": "Example nextflow script\nThe nf-nest repo contains a small example nextflow script, which we will use to demonstrate the two ways to launch an experiment. Here is the script for reference:\n\nworkflow  {\n    hello()\n}\n\nprocess hello {\n    debug true \n    \"\"\"\n    echo Hello world!\n    \"\"\"\n}",
    "crumbs": [
      "Launch an experiment"
    ]
  },
  {
    "objectID": "04_launch.html#local-execution",
    "href": "04_launch.html#local-execution",
    "title": "Launching an experiment",
    "section": "Local execution",
    "text": "Local execution\nUse the following command to run the nextflow script locally:\n\ncd experiment_repo\n./nextflow run nf-nest/examples/hello.nf\n\nN E X T F L O W  ~  version 25.04.6\nWARN: It appears you have never run this project before -- Option `-resume` is ignored\nLaunching `nf-nest/examples/hello.nf` [small_rosalind] DSL2 - revision: 9d1a692a7e\n[54/ab9974] Submitted process &gt; hello\nHello world!",
    "crumbs": [
      "Launch an experiment"
    ]
  },
  {
    "objectID": "04_launch.html#cluster-execution",
    "href": "04_launch.html#cluster-execution",
    "title": "Launching an experiment",
    "section": "Cluster execution",
    "text": "Cluster execution\nTo run on a cluster, add the argument -profile cluster which instructs nextflow to use the configs in section cluster { ... } of the file nextflow.config created in the setup instructions.1\nHere is a minimal example:\n\n./nextflow run nf-nest/examples/hello.nf -profile cluster \n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/examples/hello.nf` [boring_ride] DSL2 - revision: 9d1a692a7e\n[54/ab9974] Cached process &gt; hello\nHello world!",
    "crumbs": [
      "Launch an experiment"
    ]
  },
  {
    "objectID": "04_launch.html#managing-long-executions",
    "href": "04_launch.html#managing-long-executions",
    "title": "Launching an experiment",
    "section": "Managing long executions",
    "text": "Managing long executions\nWhen starting a job, the launching nextflow process needs to stay alive until the end of the last job. However, when the SSH connection is lost (e.g., you close your laptop), the nextflow process and hence child jobs will be killed. To avoid this, use screen or tmux which allows you to preserve a SSH session even if the SSH connection is closed.\n\nBasic instructions to start a long job\n\nTake note of which of the login nodes you are starting the job (e.g. in Sockeye, login01, login02 or login03).\nList existing tmux sessions with tmux ls\nCreate one with tmux or join last one with tmux a\nStart the nextflow.\nTo detach the tmux session ctrl-b followed by d\n\n\n\nLook at the status after being disconnected\n\nSSH in.\nIf you are in the wrong login node, use e.g., ssh login02 (follow 2FA instruction)\nReattach with tmux a\n\nFor more information on tmux, use this cheat sheet.\n\n\nCancelling a nextflow workflow\nPress ctrl-c only once. This will instruct nextflow to clean up children jobs: killing running jobs, as well as cancelling queued jobs.",
    "crumbs": [
      "Launch an experiment"
    ]
  },
  {
    "objectID": "04_launch.html#footnotes",
    "href": "04_launch.html#footnotes",
    "title": "Launching an experiment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the previous section (“local execution”), where we did not specify a -profile, the default profile named standard is used.↩︎",
    "crumbs": [
      "Launch an experiment"
    ]
  },
  {
    "objectID": "02_setup.html",
    "href": "02_setup.html",
    "title": "Setting up your HPC account",
    "section": "",
    "text": "You will need to go through these setup instructions before using nf-nest. This needs to be done only once per user and per cluster.",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#overview",
    "href": "02_setup.html#overview",
    "title": "Setting up your HPC account",
    "section": "",
    "text": "You will need to go through these setup instructions before using nf-nest. This needs to be done only once per user and per cluster.",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#prerequisites",
    "href": "02_setup.html#prerequisites",
    "title": "Setting up your HPC account",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nWorking knowledge of unix (files, permissions, processes, environment variables).\nWorking knowledge of git.\n\nThere are countless web resources to learn those things. Tedious but worth the investment.",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#optional-password-less-ssh",
    "href": "02_setup.html#optional-password-less-ssh",
    "title": "Setting up your HPC account",
    "section": "Optional: Password-less SSH",
    "text": "Optional: Password-less SSH\nIt is imperative for your sanity to avoid entering your password and do 2FA every time a SSH connection is needed.\nFor example, on Sockeye, the closest you can get to that is with SSH Connection Sharing. This means that you open a first terminal window and perform 2FA in it. As long as that window is open, other SSH connections can be established without password and 2FA. Minimize that first window and do not touch it.",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#optional-install-vs-code",
    "href": "02_setup.html#optional-install-vs-code",
    "title": "Setting up your HPC account",
    "section": "Optional: Install VS Code",
    "text": "Optional: Install VS Code\nVS Code makes it easier to develop on a remote server. For example, it simplifies file editing, resuming your session, manages github credential for you, etc.\n\nOn your laptop, install VSCode.\nFollow two steps documented in the VS Code website\n\nInstall the SSH extension\nClick on the Remote SSH icon in the lower left, this will let you Connect to Remote Host.\n\n\nUse New Terminal to open a terminal. Once you have a project (git repo) where you develop ready, use Open Folder... to point VS Code to the root of your project. VS Code is also able to manage your github authentification.",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#understanding-bash",
    "href": "02_setup.html#understanding-bash",
    "title": "Setting up your HPC account",
    "section": "Understanding bash",
    "text": "Understanding bash\nWhen you login to HPC, it starts a process allowing you to interact with the server in a scripting language, which is typically bash.\nIt is a good investment to learn the basics of bash. See the software carpentry open source tutorial.",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#understanding-environment-variables",
    "href": "02_setup.html#understanding-environment-variables",
    "title": "Setting up your HPC account",
    "section": "Understanding environment variables",
    "text": "Understanding environment variables\nAny UNIX process owns a special set of global variables called its environment variables. Each variable holds a string, possibly empty.\nAn important convention is the PATH environment variable holding a :-separated list of paths where executable such as julia or python will be searched for.\nFor example, in a bash process, to display the value of an environment variable called MY_ENV_VAR, use\necho $MY_ENV_VAR\nIn a Julia process:\nENV[\"MY_ENV_VAR\"]\nWhen a process starts a child process, the parent process decides what the child’s environment variable will be.\nFor example, consider what happens when you start Julia from bash. It is bash that will decide. Bash does it as follows: it uses the command export to mark a variable as something we want to propagate to children. This is why you will see export PATH in a start up file (next section).\nNextflow will take a different approach than bash: the environment variables are specified explicitly to ensure better reproducibility.",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#understanding-start-up-files",
    "href": "02_setup.html#understanding-start-up-files",
    "title": "Setting up your HPC account",
    "section": "Understanding start up files",
    "text": "Understanding start up files\nBash is used in several different ways, with terminology for each kind.\nIn one axis: bash is called either on login or not:\n\nlogin: when you login to a machine (via SSH, or start-up a linux machine).\nnon-login after being logged in, bash can be called by another process (e.g. bash, VS Code).\n\nIn another axis: bash is called either:\n\ninteractively: when a user will interact with the process\nnon-interactively: when it is another program in control.\n\nThe following files will be sourced automatically in the following circumstances:\n\n~/.bashrc, loaded when the shell is interactive and non-login,\n~/.bash_profile, loaded for login shell.\n\nA common pattern is to use the following in ~/.bash_profile\n# put that in .bash_profile so that anything in .bashrc also gets loaded\nif [ -f ~/.bashrc ]; then\n    . ~/.bashrc\nfi\nand then to put environment variables in ~/.bash_profile and aliases in ~/.bashrc.\nRecall that any environment variables with export are passed in to child process. So no need to set them in non-login shells. On the other hand, anything with alias is not propagated to a child process.\nOne annoyance with this strategy is that after an update of the PATH variable, if you have several windows open, you may have to logout and log back in for the change to take effect. Moreover, even if you disconnect to VSCode, it maintains a process on the server, so you have to kill that server using\nps aux | grep .vscode-server | awk '{print $2}' | xargs kill\nthen log out, and then log back in.",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#git",
    "href": "02_setup.html#git",
    "title": "Setting up your HPC account",
    "section": "Git",
    "text": "Git\nWe assume the command git is available.\nIn some HPC such as UBC Sockeye, this command is not available by default, instead a module has to be loaded:\n\nmodule load git",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#allocation-code",
    "href": "02_setup.html#allocation-code",
    "title": "Setting up your HPC account",
    "section": "Allocation code",
    "text": "Allocation code\nIn some HPC such as UBC Sockeye, we need an allocation code to submit to the job queue. Scripts in nf-nest use an ENV variable called ALLOCATION_CODE to find the allocation code.\nTo set the variable in your current session, use:\n\nexport ALLOCATION_CODE=my-alloc-code",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#optional-disable-nextflow-fancy-output",
    "href": "02_setup.html#optional-disable-nextflow-fancy-output",
    "title": "Setting up your HPC account",
    "section": "Optional: disable nextflow fancy output",
    "text": "Optional: disable nextflow fancy output\nIt can be useful to avoid nextflow’s fancy progress output, for example they do not work well in notebook or in screen.\nTo disable in the current session, use:\n\nexport NXF_ANSI_LOG=false",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#java",
    "href": "02_setup.html#java",
    "title": "Setting up your HPC account",
    "section": "Java",
    "text": "Java\nNextflow needs Java. Follow these instructions taken from the nextflow website:\nFirst, Install SDKMAN\ncurl -s https://get.sdkman.io | bash\nSecond, open a new terminal, and install Java:\nsdk install java 17.0.10-tem\nFinally, confirm that Java in installed correctly:\njava -version",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#install-julia",
    "href": "02_setup.html#install-julia",
    "title": "Setting up your HPC account",
    "section": "Install Julia",
    "text": "Install Julia\nYou can install Julia using the “Linux and FreeBSD” instructions. For example using the URL for the Long-term support version 1.10.6.\nYou do not need root privileges, e.g., install in a folder at ~/bin/ and add to your path.",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#julia-depot",
    "href": "02_setup.html#julia-depot",
    "title": "Setting up your HPC account",
    "section": "Julia depot",
    "text": "Julia depot\nJulia has a package manager called Pkg. The Julia depot folder is a location where Pkg puts files it downloads and compiles. Each user should have their own. The path to the depot is controlled via the JULIA_DEPOT_PATH environment variable.\nIn a HPC setup, the JULIA_DEPOT_PATH should be accessible read/write by all nodes, and ideally be on fast storage.\nFor example, on Sockeye the first choice if you allocation has it would be to use burst storage, i.e. path of the form\nexport JULIA_DEPOT_PATH=/arc/burst/[allocation_code]/[username_in_alloc]/depot\nA second choice would be to use the scratch space\nexport JULIA_DEPOT_PATH=/scratch/[allocation_code]/[username_in_alloc]/depot",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "02_setup.html#optional-useful-aliases",
    "href": "02_setup.html#optional-useful-aliases",
    "title": "Setting up your HPC account",
    "section": "Optional: useful aliases",
    "text": "Optional: useful aliases\nBash supports shortcuts called alias for commonly used commands. We show two use cases here.\nA very common patter in Julia is to activate the environment in the current directory and load Revise.jl. To do so, create a Julia script in your home directory called julia-start.jl and put that in it:\nprintln(\"Active project: $(Base.active_project())\")\nprintln(\"Loading Revise...\")\nusing Revise\nThen add the following to .bashrc:\nalias j='julia --banner=no --load /home/[your_alloc]/julia-start.jl --project=@. ' \nYou will need to install Revise.jl in the global environment:\njulia\n]\nactivate\nadd Revise \nSecond, most HPC has some ways to start an interactive allocation. Create an alias to be able to do it quicly. Here is an example for interactive CPU and GPU nodes respectively on Sockeye:\nalias interact='salloc --time=3:00:00 --mem=16G --nodes=1 --ntasks=2 --account=[your_alloc]'\nalias ginteract='salloc --account=[your_alloc-gpu] --partition=interactive_gpu --time=3:00:00 -N 1 -n 2 --mem=16G --gpus=1'",
    "crumbs": [
      "Setup your HPC account"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "Intro",
    "section": "",
    "text": "Large scale numerical experiments are central to much of contemporary scientific and mathematical research. Performing these numerical experiments in a valid, reproducible and scalable fashion is not easy. Even a small typical project may need to run 1000s of executions, and 10k+ is not uncommon. It is crucial to have good tools to coordinate and organize these experiments.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_intro.html#context",
    "href": "01_intro.html#context",
    "title": "Intro",
    "section": "",
    "text": "Large scale numerical experiments are central to much of contemporary scientific and mathematical research. Performing these numerical experiments in a valid, reproducible and scalable fashion is not easy. Even a small typical project may need to run 1000s of executions, and 10k+ is not uncommon. It is crucial to have good tools to coordinate and organize these experiments.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_intro.html#nf-nest",
    "href": "01_intro.html#nf-nest",
    "title": "Intro",
    "section": "nf-nest",
    "text": "nf-nest\nThis webpage documents nf-nest, a collection of small but powerful utilities built on nextflow to help accomplish this. Link to github repository..\nAspects taken into account:\n\nAutomating cross-product of input parameters in experiments.\nAutomating creation of submission scripts and ordering of jobs (taking care of moving across file system, dynamic memory requirements, etc).\nAutomating the gathering of results from many runs.\nCaching already ran jobs.\nRobustness to failure.\nReproducibility via apptainer and docker, supporting both x86 and apple silicon.\nSupport for GPU programming.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_intro.html#technology-stack",
    "href": "01_intro.html#technology-stack",
    "title": "Intro",
    "section": "Technology stack",
    "text": "Technology stack\nnf-nest uses the following open source projects:\n\nNextflow: can be thought of as an “operating system” for coordinating numerical experiments.\nJulia: a programming language to unlock full access to high performance computation on both CPUs and GPUs.\n\nWhile some features of nf-nest are Julia specific, other parts are language agnostic.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_intro.html#background",
    "href": "01_intro.html#background",
    "title": "Intro",
    "section": "Background",
    "text": "Background\n\nScientific workflow\nA scientific workflow is a directed acyclic graph where each node is a process and each edge between node \\(n\\) to \\(n'\\) denote that at least one output of process \\(n\\) is fed as an input to process \\(n'\\).\nHere is an example from a workflow covered later in this tutorial:\n\n\n\n\n\nflowchart TB\n    subgraph \" \"\n    v0[\"julia_env\"]\n    v1[\"toml\"]\n    v4[\"Channel.fromList\"]\n    v7[\"julia_env\"]\n    v8[\"toml\"]\n    v12[\"plot_script\"]\n    end\n    v2([instantiate_process])\n    v3([precompile])\n    v5([run_julia])\n    subgraph combine_workflow\n    v9([instantiate_process])\n    v10([precompile])\n    v11([combine_process])\n    end\n    v13([plot])\n    subgraph \" \"\n    v14[\"combined_csvs_folder\"]\n    v15[\" \"]\n    end\n    v6(( ))\n    v0 --&gt; v2\n    v1 --&gt; v2\n    v2 --&gt; v3\n    v3 --&gt; v5\n    v3 --&gt; v13\n    v4 --&gt; v5\n    v5 --&gt; v6\n    v7 --&gt; v9\n    v8 --&gt; v9\n    v9 --&gt; v10\n    v10 --&gt; v11\n    v6 --&gt; v11\n    v11 --&gt; v13\n    v12 --&gt; v13\n    v13 --&gt; v15\n    v13 --&gt; v14\n\n\n\n\n\n\nIn a nutshell, nextflow will submit one or several SLURM job for each node in this graph, gather results, and produce some nice reports.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_intro.html#more-information",
    "href": "01_intro.html#more-information",
    "title": "Intro",
    "section": "More information",
    "text": "More information\nBoth nextflow and Julia have excellent and extensive documentation.\n\nNexflow documentation\nJulia documentation\n\nSee also this nextflow tutorial.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "03_create-exp-repo.html",
    "href": "03_create-exp-repo.html",
    "title": "Creating an experiment repo",
    "section": "",
    "text": "We use the term “experiment repo” for a git repository that contains the plumbing required for a numerical experiment.\nIt consists in a git repository with:\n\nthe nextflow executable and its configuration, nextflow.config;\na git submodule, nf-nest containing useful tools;\nyour .nf files and other supporting files.\n\nThis pages explains how to setup an experiment repo.",
    "crumbs": [
      "Create an experiment repo"
    ]
  },
  {
    "objectID": "03_create-exp-repo.html#overview",
    "href": "03_create-exp-repo.html#overview",
    "title": "Creating an experiment repo",
    "section": "",
    "text": "We use the term “experiment repo” for a git repository that contains the plumbing required for a numerical experiment.\nIt consists in a git repository with:\n\nthe nextflow executable and its configuration, nextflow.config;\na git submodule, nf-nest containing useful tools;\nyour .nf files and other supporting files.\n\nThis pages explains how to setup an experiment repo.",
    "crumbs": [
      "Create an experiment repo"
    ]
  },
  {
    "objectID": "03_create-exp-repo.html#location",
    "href": "03_create-exp-repo.html#location",
    "title": "Creating an experiment repo",
    "section": "Location",
    "text": "Location\nIn an HPC context, the experiment repo should be in a read/write location accessible to all nodes.\nFor example, on Sockeye the first choice if you allocation has it would be to use burst storage, i.e. cd to a path of the form\n/arc/burst/[allocation_code]/[username_in_alloc]/\nA second choice would be to use the scratch space, i.e. cd to a path of the form\n/scratch/[allocation_code]/[username_in_alloc]/",
    "crumbs": [
      "Create an experiment repo"
    ]
  },
  {
    "objectID": "03_create-exp-repo.html#sec-instructions",
    "href": "03_create-exp-repo.html#sec-instructions",
    "title": "Creating an experiment repo",
    "section": "Instructions",
    "text": "Instructions\nFor quick setup, run the following commands:\n\n# create a directory and cd into it\nmkdir experiment_repo && cd $_\n\ngit init\n\n# setup nextflow\ncurl -s https://get.nextflow.io | bash\n\n# add nf-nest utilities\ngit submodule add https://github.com/UBC-Stat-ML/nf-nest.git\n\n# copy template for nextflow configurations\ncp nf-nest/nextflow.config .",
    "crumbs": [
      "Create an experiment repo"
    ]
  },
  {
    "objectID": "05_pkg.html",
    "href": "05_pkg.html",
    "title": "Managing Julia packages on HPC",
    "section": "",
    "text": "Installing a package in Julia includes two steps (1) downloading (2) pre-compiling. In a laptop, both are done in one shot using\n]\nactivate experiment_repo/julia_env\nadd NameOfPackage\nIn an HPC setup, step (2) requires computational effort hence is best done in compute node. However in some HPC setups such as Sockeye, compute nodes do not have internet access, so the process needs to be split. We cover here some utilities that facilitate this.",
    "crumbs": [
      "Add Julia packages on HPC"
    ]
  },
  {
    "objectID": "05_pkg.html#overview",
    "href": "05_pkg.html#overview",
    "title": "Managing Julia packages on HPC",
    "section": "",
    "text": "Installing a package in Julia includes two steps (1) downloading (2) pre-compiling. In a laptop, both are done in one shot using\n]\nactivate experiment_repo/julia_env\nadd NameOfPackage\nIn an HPC setup, step (2) requires computational effort hence is best done in compute node. However in some HPC setups such as Sockeye, compute nodes do not have internet access, so the process needs to be split. We cover here some utilities that facilitate this.",
    "crumbs": [
      "Add Julia packages on HPC"
    ]
  },
  {
    "objectID": "05_pkg.html#julia-environment",
    "href": "05_pkg.html#julia-environment",
    "title": "Managing Julia packages on HPC",
    "section": "Julia environment",
    "text": "Julia environment\nA Julia environment is a specification of all package dependencies and their versions.\nBy convention, we will store it in a directory called experiment_repo/julia_env.",
    "crumbs": [
      "Add Julia packages on HPC"
    ]
  },
  {
    "objectID": "05_pkg.html#adding-a-package",
    "href": "05_pkg.html#adding-a-package",
    "title": "Managing Julia packages on HPC",
    "section": "Adding a package",
    "text": "Adding a package\nThere are two syntaxes in Julia to install packages: the interactive Julia package manager or using programmatic Pkg syntax. We cover both below.\nIn both syntaxes, we first need to tell Julia which environment to use. This is called “activating” an environment.\n\nInteractive Julia package manager\nThe most common method is to use the interactive Julia package manager. To start it, type ] followed by enter.\nActivating is done with the activate keyword.\nYou can then add a package using the add command. The argument of add can be either a registered Julia package, for example we show here how to add the Pigeons package for MCMC sampling:\nENV[\"JULIA_PKG_PRECOMPILE_AUTO\"]=0 # Hold off precompile since we are in login node\n]\nactivate experiment_repo/julia_env\nadd Pigeons\nAlternatively, the package can be a git repository, optionally with a specific commit/tag:\nadd Example@0.5\nadd Example#master\nadd Example#c37b675\nadd https://github.com/JuliaLang/Example.jl#master\nadd git@github.com:JuliaLang/Example.jl.git\nadd \"git@github.com:JuliaLang/Example.jl.git\"#master\nadd https://github.com/Company/MonoRepo:juliapkgs/Package.jl\nTo exit the interactive Julia package manager, type control-C.\n\n\nProgrammatic interface\nAlternatively, a programmatic interface is also available for scripting:\n\nENV[\"JULIA_PKG_PRECOMPILE_AUTO\"]=0 # Hold off precompile since we are in login node\nusing Pkg \nPkg.activate(\"experiment_repo/julia_env\")\nPkg.add(\"Pigeons\")",
    "crumbs": [
      "Add Julia packages on HPC"
    ]
  },
  {
    "objectID": "05_pkg.html#precompilation-on-hpc",
    "href": "05_pkg.html#precompilation-on-hpc",
    "title": "Managing Julia packages on HPC",
    "section": "Precompilation on HPC",
    "text": "Precompilation on HPC\nOnce we have downloaded the packages in the login node, we now turn to the task of performing pre-compilation.\n\ncd experiment_repo \n./nextflow run nf-nest/pkg.nf -profile cluster\n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/pkg.nf` [romantic_engelbart] DSL2 - revision: fc0374e695\n[39/9792b7] Submitted process &gt; instantiate_process\n[dc/ff1a99] Submitted process &gt; precompile\n\n\nOptionally, you can append an argument to specify the number of threads to request and use during pre-compilation: e.g. add --nPrecompileThreads 20 to request 20 threads instead of the default of 10.",
    "crumbs": [
      "Add Julia packages on HPC"
    ]
  },
  {
    "objectID": "05_pkg.html#testing-your-julia-environment-interactively",
    "href": "05_pkg.html#testing-your-julia-environment-interactively",
    "title": "Managing Julia packages on HPC",
    "section": "Testing your Julia environment interactively",
    "text": "Testing your Julia environment interactively\nIn the code above, we have added the package Pigeons as an example.\nTo interactively test an installed package, simply activate the environment from a Julia session:\n\nusing Pkg\nPkg.activate(\"experiment_repo/julia_env\")\n\nusing Pigeons\npigeons(target = toy_mvn_target(1000))",
    "crumbs": [
      "Add Julia packages on HPC"
    ]
  },
  {
    "objectID": "05_pkg.html#more-information",
    "href": "05_pkg.html#more-information",
    "title": "Managing Julia packages on HPC",
    "section": "More information",
    "text": "More information\nSee Pkg.jl documentation.",
    "crumbs": [
      "Add Julia packages on HPC"
    ]
  },
  {
    "objectID": "07_combine.html",
    "href": "07_combine.html",
    "title": "Combine outputs",
    "section": "",
    "text": "Now that we know how to run many jobs, the next question is how to combine the output of all these jobs to analyze it.",
    "crumbs": [
      "Combine outputs"
    ]
  },
  {
    "objectID": "07_combine.html#overview",
    "href": "07_combine.html#overview",
    "title": "Combine outputs",
    "section": "",
    "text": "Now that we know how to run many jobs, the next question is how to combine the output of all these jobs to analyze it.",
    "crumbs": [
      "Combine outputs"
    ]
  },
  {
    "objectID": "07_combine.html#example",
    "href": "07_combine.html#example",
    "title": "Combine outputs",
    "section": "Example",
    "text": "Example\nWe will run Pigeons on the cross product formed by calling crossProduct(variables) with:\ndef variables = [\n    seed: 1..10,\n    n_chains: [10, 20], \n]\nSuppose we want to create a plot from the output of these 20 Julia processes.",
    "crumbs": [
      "Combine outputs"
    ]
  },
  {
    "objectID": "07_combine.html#strategy",
    "href": "07_combine.html#strategy",
    "title": "Combine outputs",
    "section": "Strategy",
    "text": "Strategy\nEach Julia process will create a folder. Using a function, we will provide an automatic name to this folder encoding the inputs used (seed and n_chains). That name is provided by nf-nest’s filed() function. In that folder, we will\nput csv files.\nThen, once all Julia processes are done, another utilities from nf-nest, combine_csvs, will merge all CSVs while adding columns for the inputs (here, seed and n_chains).\nFinally, we will pass the merged CSVs to a plotting process.",
    "crumbs": [
      "Combine outputs"
    ]
  },
  {
    "objectID": "07_combine.html#nextflow-script",
    "href": "07_combine.html#nextflow-script",
    "title": "Combine outputs",
    "section": "Nextflow script",
    "text": "Nextflow script\n\n// includes are relative to the .nf file, should always start with ./ or ../\ninclude { crossProduct; filed; deliverables } from '../cross.nf'\ninclude { instantiate; precompile; activate } from '../pkg.nf'\ninclude { combine_csvs; } from '../combine.nf'\n\n// in contrast, file(..) is relative to `pwd`, use projectDir/ \n//   to make it relative to main .nf file, or moduleDir for the .nf file\ndef julia_env = file(moduleDir/'julia_env')\ndef plot_script = file(moduleDir/'plot.jl')\n\ndef variables = [\n    seed: 1..10,\n    n_chains: [10, 20], \n]\n\nworkflow {\n    compiled_env = instantiate(julia_env) | precompile\n    configs = crossProduct(variables)\n    combined = run_julia(compiled_env, configs) | combine_csvs\n    plot(compiled_env, plot_script, combined)\n}\n\nprocess run_julia {\n    input:\n        path julia_env \n        val config \n    output:\n        path \"${filed(config)}\"\n    \"\"\"\n    ${activate(julia_env)}\n\n    # run your code\n    using Pigeons \n    using CSV \n    pt = pigeons(\n            target = toy_mvn_target(1000), \n            n_chains = ${config.n_chains}, \n            seed = ${config.seed})\n\n    # organize output as follows:\n    #   - create a directory with name controlled by filed(config)\n    #     to keep track of input configuration\n    #   - put any number of CSV in there\n    mkdir(\"${filed(config)}\")\n    CSV.write(\"${filed(config)}/summary.csv\", pt.shared.reports.summary)\n    CSV.write(\"${filed(config)}/swap_prs.csv\", pt.shared.reports.swap_prs)\n    \"\"\"\n}\n\nprocess plot {\n    input:\n        path julia_env \n        path plot_script\n        path combined_csvs_folder \n    output:\n        path '*.png'\n        path combined_csvs_folder\n    publishDir \"${deliverables(workflow, params)}\", mode: 'copy', overwrite: true\n    \"\"\"\n    ${activate(julia_env)}\n\n    include(\"$plot_script\")\n    create_plots(\"$combined_csvs_folder\")\n    \"\"\"\n}",
    "crumbs": [
      "Combine outputs"
    ]
  },
  {
    "objectID": "07_combine.html#running-the-nextflow-script",
    "href": "07_combine.html#running-the-nextflow-script",
    "title": "Combine outputs",
    "section": "Running the nextflow script",
    "text": "Running the nextflow script\n\ncd experiment_repo\n./nextflow run nf-nest/examples/full.nf -profile cluster \n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/examples/full.nf` [peaceful_planck] DSL2 - revision: a68c131baa\n[7f/c48687] Submitted process &gt; instantiate_process\n[a1/01de97] Submitted process &gt; combine_workflow:instantiate_process\n[62/e5f07c] Submitted process &gt; combine_workflow:precompile\n[18/b3eff4] Submitted process &gt; precompile\n[e1/4a97fb] Submitted process &gt; run_julia (1)\n[94/2bb620] Submitted process &gt; run_julia (12)\n[fd/c65f33] Submitted process &gt; run_julia (8)\n[fe/8349e2] Submitted process &gt; run_julia (5)\n[86/d2188b] Submitted process &gt; run_julia (11)\n[dd/a1da28] Submitted process &gt; run_julia (10)\n[1f/e7cb21] Submitted process &gt; run_julia (7)\n[2e/1f9bba] Submitted process &gt; run_julia (6)\n[df/cd6375] Submitted process &gt; run_julia (9)\n[1b/a2c88c] Submitted process &gt; run_julia (4)\n[78/d434da] Submitted process &gt; run_julia (3)\n[66/9afc5a] Submitted process &gt; run_julia (13)\n[a8/b2cf25] Submitted process &gt; run_julia (2)\n[12/b66506] Submitted process &gt; run_julia (14)\n[6b/75f684] Submitted process &gt; run_julia (16)\n[41/96a420] Submitted process &gt; run_julia (15)\n[9a/bb5545] Submitted process &gt; run_julia (17)\n[0c/fbbb9a] Submitted process &gt; run_julia (20)\n[82/949f45] Submitted process &gt; run_julia (18)\n[99/a84917] Submitted process &gt; run_julia (19)\n[71/2877e4] Submitted process &gt; combine_workflow:combine_process\n[0a/b73e43] Submitted process &gt; plot",
    "crumbs": [
      "Combine outputs"
    ]
  },
  {
    "objectID": "07_combine.html#accessing-the-output",
    "href": "07_combine.html#accessing-the-output",
    "title": "Combine outputs",
    "section": "Accessing the output",
    "text": "Accessing the output\nEach nextflow process is associated with a unique work directory to ensure the processes do not interfere with each other. Here we cover two ways to quickly access these work directories.\n\nQuick inspection\nA quick way to find the output of a nextflow process that we just ran is to use:\ncd experiment_repo \nnf-nest/nf-open\nThis lists the work folders for the last nextflow job.\n\n\nOrganizing the output with a publishDir\nA better approach is to use the publishDir directive, combined with nf-nest’s deliverables() utility, as illustrated in the run_julia process above. This will automatically copy the output of the process associated with the directive in a sub-directory of experiment_repo/deliverables.\n\ncd experiment_repo\ntree deliverables\n\ndeliverables\n└── scriptName=full.nf\n    ├── output\n    │   ├── summary.csv\n    │   └── swap_prs.csv\n    ├── plot.png\n    └── runName.txt\n\n2 directories, 4 files\n\n\nHere the contents of runName.txt can be used with nextflow’s log command to obtain more information on the run.\n\ncat deliverables/scriptName=full.nf/runName.txt \n\npeaceful_planck\n\n\n\n./nextflow log\n\nTIMESTAMP           DURATION    RUN NAME            STATUS  REVISION ID SESSION ID                              COMMAND                                                    \n2025-07-03 17:01:17 2.9s        small_rosalind      OK      9d1a692a7e  d108fc2d-a202-40d8-b7ca-4e9fc18872fb    nextflow run nf-nest/examples/hello.nf                     \n2025-07-03 17:01:23 7.3s        boring_ride         OK      9d1a692a7e  d108fc2d-a202-40d8-b7ca-4e9fc18872fb    nextflow run nf-nest/examples/hello.nf -profile cluster    \n2025-07-03 17:02:17 1m 18s      romantic_engelbart  OK      fc0374e695  d108fc2d-a202-40d8-b7ca-4e9fc18872fb    nextflow run nf-nest/pkg.nf -profile cluster               \n2025-07-03 17:03:59 1m 48s      tiny_lagrange       OK      aa082b1978  d108fc2d-a202-40d8-b7ca-4e9fc18872fb    nextflow run nf-nest/examples/many_jobs.nf -profile cluster\n2025-07-03 17:05:50 5.2s        gigantic_heisenberg OK      d9de661ecc  d108fc2d-a202-40d8-b7ca-4e9fc18872fb    nextflow run nf-nest/examples/filter.nf                    \n2025-07-03 17:06:01 6m 24s      peaceful_planck     OK      a68c131baa  d108fc2d-a202-40d8-b7ca-4e9fc18872fb    nextflow run nf-nest/examples/full.nf -profile cluster     \n\n\nAnd we can see in the CSV that indeed the columns seed and n_chains were added to the left:\n\nhead -n 2 deliverables/scriptName=full.nf/output/summary.csv \n\nseed,n_chains,round,n_scans,n_tempered_restarts,global_barrier,global_barrier_variational,last_round_max_time,last_round_max_allocation,stepping_stone\n10,10,1,2,,8.998207738433418,,0.000233774,13536.0,-1173.429270641805",
    "crumbs": [
      "Combine outputs"
    ]
  },
  {
    "objectID": "09_gpu.html",
    "href": "09_gpu.html",
    "title": "GPU programming",
    "section": "",
    "text": "In a context such as Sockeye where it is not possible to access GPU nodes with internet access, precompilation becomes more complicated than the earlier page on non-CUDA precompilation.\nWe provide a workaround, pkg_gpu.nf, which offers the same functionality as pkg.nf but is slower since all precompilation has to occur on the login node.\nFirst, add the package as before:\n\nENV[\"JULIA_PKG_PRECOMPILE_AUTO\"]=0 # Hold off precompile since we are in login node\nusing Pkg \nPkg.activate(\"experiment_repo/julia_env\")\nPkg.add(\"CUDA\")\n\nNext, use the GPU precompilation script:\n\ncd experiment_repo \n./nextflow run nf-nest/pkg_gpu.nf \n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/pkg_gpu.nf` [zen_payne] DSL2 - revision: 1db0d8dc5b\n[bb/60b094] Submitted process &gt; instantiate_process\n[98/f80df2] Submitted process &gt; precompile_gpu",
    "crumbs": [
      "GPU programming"
    ]
  },
  {
    "objectID": "09_gpu.html#installing-cuda.jl",
    "href": "09_gpu.html#installing-cuda.jl",
    "title": "GPU programming",
    "section": "",
    "text": "In a context such as Sockeye where it is not possible to access GPU nodes with internet access, precompilation becomes more complicated than the earlier page on non-CUDA precompilation.\nWe provide a workaround, pkg_gpu.nf, which offers the same functionality as pkg.nf but is slower since all precompilation has to occur on the login node.\nFirst, add the package as before:\n\nENV[\"JULIA_PKG_PRECOMPILE_AUTO\"]=0 # Hold off precompile since we are in login node\nusing Pkg \nPkg.activate(\"experiment_repo/julia_env\")\nPkg.add(\"CUDA\")\n\nNext, use the GPU precompilation script:\n\ncd experiment_repo \n./nextflow run nf-nest/pkg_gpu.nf \n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/pkg_gpu.nf` [zen_payne] DSL2 - revision: 1db0d8dc5b\n[bb/60b094] Submitted process &gt; instantiate_process\n[98/f80df2] Submitted process &gt; precompile_gpu",
    "crumbs": [
      "GPU programming"
    ]
  },
  {
    "objectID": "09_gpu.html#running-nextflow-processes-requiring-gpu",
    "href": "09_gpu.html#running-nextflow-processes-requiring-gpu",
    "title": "GPU programming",
    "section": "Running nextflow processes requiring GPU",
    "text": "Running nextflow processes requiring GPU\nAn example of a workflow using GPUs:\n\ninclude { instantiate; precompile_gpu; } from \"../pkg_gpu.nf\"\ninclude { activate; } from \"../pkg.nf\"\n\ndef julia_env = file('julia_env')\n\nworkflow {\n    instantiate(julia_env) | precompile_gpu | run_julia\n}\n\nprocess run_julia {\n    debug true\n    label 'gpu'\n    input:\n        path julia_env\n    \"\"\"\n    ${activate(julia_env)}\n\n    using CUDA \n\n    println(\"CPU\")\n    x = rand(5000, 5000);\n    @time x * x;\n    @time x * x;\n\n    println(\"GPU\")\n    x = CUDA.rand(5000, 5000);\n    @time x * x;\n    @time x * x;\n\n    CUDA.versioninfo()\n    \"\"\"\n}\n\nWe run it using the same command as usual:\n\ncd experiment_repo\n./nextflow run nf-nest/examples/gpu.nf -profile cluster\n\nN E X T F L O W  ~  version 25.04.6\nLaunching `nf-nest/examples/gpu.nf` [suspicious_shannon] DSL2 - revision: 9be41cea49\n[b3/ad92cc] Submitted process &gt; instantiate_process\n[02/164d10] Submitted process &gt; precompile_gpu\n[a6/d78d9b] Submitted process &gt; run_julia\nCPU\n 13.212915 seconds (2.08 M allocations: 331.278 MiB, 2.01% gc time, 12.01% compilation time)\n 11.711777 seconds (2 allocations: 190.735 MiB, 1.72% gc time)\nGPU\n  2.219103 seconds (2.69 M allocations: 183.856 MiB, 97.33% compilation time)\n  0.000629 seconds (71 allocations: 1.625 KiB)\nCUDA runtime 12.5, artifact installation\nCUDA driver 12.9\nNVIDIA driver 550.163.1\n\nCUDA libraries: \n- CUBLAS: 12.5.3\n- CURAND: 10.3.6\n- CUFFT: 11.2.3\n- CUSOLVER: 11.6.3\n- CUSPARSE: 12.5.1\n- CUPTI: 2024.2.1 (API 23.0.0)\n- NVML: 12.0.0+550.163.1\n\nJulia packages: \n- CUDA: 5.8.2\n- CUDA_Driver_jll: 0.13.1+0\n- CUDA_Runtime_jll: 0.17.1+0\n\nToolchain:\n- Julia: 1.10.8\n- LLVM: 15.0.7\n\nPreferences:\n- CUDA_Runtime_jll.version: 12.5\n\n1 device:\n  0: Tesla V100-SXM2-32GB (sm_70, 31.137 GiB / 32.000 GiB available)",
    "crumbs": [
      "GPU programming"
    ]
  },
  {
    "objectID": "09_gpu.html#gpu-kernel-development",
    "href": "09_gpu.html#gpu-kernel-development",
    "title": "GPU programming",
    "section": "GPU kernel development",
    "text": "GPU kernel development\nOne way to leverage GPUs is to use array programming as demonstrated in the example above. When a problem cannot be cast into an array problem, an alternative is to construct a custom GPU kernel.\nDesigning custom GPU kernels is especially attractive in Julia. This is in big part thanks to\nKernelAbstractions.jl, which allows the same code to be emit both CPU and GPU versions. Since error messages are easier to interpret when doing CPU development, it is useful to be able to test both CPU and GPU targets.\nCompared to Julia CPU development, the main constraint when doing GPU development is that inside the kernel there should not be heap allocations. Seasoned Julia developers are often already often avoiding to allocate in the inner loop due to garbage collection costs.\nFor a concrete example of KernelAbstractions.jl in action, see these kernels used to implement Sequential Annealed Importance Sampling.",
    "crumbs": [
      "GPU programming"
    ]
  }
]